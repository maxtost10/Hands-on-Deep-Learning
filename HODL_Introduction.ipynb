{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS_b7lkiuFSm"
      },
      "source": [
        "Notebook created by [Peter Belcák](https://disco.ethz.ch/members/pbelcak)\n",
        "\n",
        "<br>\n",
        "\n",
        "Updated:<br>\n",
        "\n",
        "For FS24 by [Andreas Plesner](https://disco.ethz.ch/members/aplesner)\n",
        "\n",
        "For HS24, FS25 by [Till Aczel](https://disco.ethz.ch/members/taczel)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPXC6lGgqhJ1"
      },
      "source": [
        "# Hand-On Deep Learing - Introduction Session\n",
        "\n",
        "This session will introduce you to the basic concepts of differentiable programming and traning neural networks from scratch. We will be using the popular deep learning framework PyTorch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TM0WBd5RpGGa"
      },
      "source": [
        "We will first talk about the bread-and-butter data type of deep learning - tensors. Once done, we will give a quick introduction to differentiable programs. On a simple example, we will show how to compute gradients in pytorch, and then lead you towards an implementation of a simple gradient descent training loop.\n",
        "\n",
        "Having got a better feel of how one trains the parameters of a differentiable program, we will introduce neural networks. We will provide you with a full implementation of a training loop, and help you train an approximation of a Boolean function.\n",
        "\n",
        "We will then steer away from illustrative examples and get started with practical, application-oriented deep learning. We will define and train both shallow and deep neural networks for image classification, tune the parameters of training and the sizes of architectures, and observe how the individual properties of our training setup influence the quality of the learning outcomes.\n",
        "\n",
        "We will conclude this session with the introduction of convolutional layers. The challenge of the day will be to use convolutional neural networks to correctly classify greyscale images of fashion articles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWg7Abwlot8z",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Prelude: Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2y1UqizqeUr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsArlqzXFU1X"
      },
      "source": [
        "For training of deep models, `torch` uses a special data type: `torch.tensor`. The `torch.tensor` is `torch`'s way to store tensors which can be seen as multidimensional arrays, with the vector and matrix simply being the 1 and 2 dimensional instances. The values in `torch.tensor` can be learned from data.\n",
        "\n",
        "Before we can move on to do anything more exciting, one has to know a bit about tensors. Anyone familiar with `numpy.ndarray` can skip to the last subsection on trainable tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggXYIWqrF5gi"
      },
      "source": [
        "#### Tensors are an enhanced, uniform variant of multi-dimensional lists that torch operations can eat."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gI8gFtZwGOHy",
        "outputId": "6db37aa7-6c49-4b39-9649-f362286ab279"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occured in matmul(): argument 'input' (position 1) must be Tensor, not list\n"
          ]
        }
      ],
      "source": [
        "A = [[ 0.0, 1.0], [ 1.0 , 0.0]]\n",
        "\n",
        "try:\n",
        "  torch.matmul(A, A) # throws a TypeError\n",
        "except TypeError as error:\n",
        "  print(f\"An error occured in {error}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkXDlE2WGfCY",
        "outputId": "8fc1c4d8-aa53-4fa9-e755-2cd9b896a1be"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 0.],\n",
              "        [0., 1.]])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "A_tensor = torch.tensor(A)\n",
        "\n",
        "torch.matmul(A_tensor, A_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9veRCZCKlqp"
      },
      "source": [
        "Notice that nested lists that are candidates for tensors must be uniform in every dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLXm_LvnLdGC",
        "outputId": "5c6146f0-6548-4a1f-9e0a-d5cf347e3cd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Could not form a tensor: expected sequence of length 3 at dim 2 (got 2)\n"
          ]
        }
      ],
      "source": [
        "B = [\n",
        "    [[1, 2, 3], [4, 5, 6]],\n",
        "    [[0, 0], [1, 1]]\n",
        "]\n",
        "\n",
        "\n",
        "try:\n",
        "  B_tensor = torch.tensor(B) # throws a ValueError\n",
        "except ValueError as error:\n",
        "  print(f\"Could not form a tensor: {error}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLtbSji2NXIB"
      },
      "source": [
        "**Exercise.** Create a tensor `I_tensor`, which is a 3x3 identity matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4a53MyvNnOZ",
        "outputId": "132528d8-19cb-4979-c31c-1a55b4defbdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1, 0, 0],\n",
            "        [0, 1, 0],\n",
            "        [0, 0, 1]])\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "I = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
        "\n",
        "I_tensor = torch.tensor(I)\n",
        "print(I_tensor@I_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8N_bgQbmHt5x"
      },
      "source": [
        "#### Tensors have a multi-dimensional `size`, also known as `shape`\n",
        "The shape of a tensor describes the sizes of its individual tensor dimensions (also known as *axes*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xp9ZCybhH9Fl",
        "outputId": "9722a828-50f0-4adb-cc63-040f7f48a14e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 2])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "A_tensor.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNgPKohDIBZm",
        "outputId": "5d131a2d-1ca1-4119-8c6b-7edcbad86ec3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 2])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "A_tensor.size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2rzIVf0IRDl"
      },
      "source": [
        "Notice that `A` consists of two lists containing two elements each.\n",
        "Correspondingly, `A_tensor` has size of `[2, 2]`, meaning that `A_tensor` consists of two sub-tensors, namely `A_tensor[0]` and `A_tensor[1]`, containing two elements each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9UQIDJMI7IZ",
        "outputId": "c4c2a4f2-40aa-4eeb-c414-c387507e0820"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1., 0.])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "A_tensor[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqgN1NydJJfC"
      },
      "source": [
        "If we take `B` such that `B` contains two lists of lists, such as in"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAJfXvqlIZra"
      },
      "outputs": [],
      "source": [
        "B = [\n",
        "    [[1, 2, 3], [4, 5, 6]],\n",
        "    [[0, 0, 0], [1, 1, 1]]\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd4A95w0JSHw"
      },
      "source": [
        "Then we can turn it into a `B_tensor` of size `[2,2,3]`,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6KrSZ3aJkzt",
        "outputId": "a4cbcbbe-612d-41ff-bdea-cf82f0d99053"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 2, 3])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "B_tensor = torch.tensor(B)\n",
        "B_tensor.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6drfln9LJrRW"
      },
      "source": [
        "... meaning that `B_tensor` consits of two two-dimensional sub-tensors, `B_tensor[0]`, and `B_tensor[1]`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1IUoaVnNwL1"
      },
      "source": [
        "**Exercise.** What should be the shape of `I_tensor`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBaWraIFN0xF"
      },
      "outputs": [],
      "source": [
        "I_tensor_shape_intended = torch.Size( [ 3, 3 ] ) # modify this line with your guess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-KzK5SbOMG2"
      },
      "source": [
        "**Exercise.** Retrieve the shape of `I_tensor`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9khgv9NORPi"
      },
      "outputs": [],
      "source": [
        "I_tensor_shape = I_tensor.shape # modify this line with the correct code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PujSmcTuOXVE"
      },
      "source": [
        "**Exercise.** Are they the same?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNlLXAAQOgj9",
        "outputId": "31f2f26e-704a-460f-e5ac-27a1d0a125af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I_tensor has shape as intended.\n"
          ]
        }
      ],
      "source": [
        "# Run this code block.\n",
        "\n",
        "if I_tensor_shape_intended == I_tensor_shape:\n",
        "  print(\"I_tensor has shape as intended.\")\n",
        "else:\n",
        "  print(\"I_tensor does not have shape as intended.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5QiVwQxKJ_R"
      },
      "source": [
        "#### You can access arbitrary sub-tensors of every tensor\n",
        "\n",
        "For this, you can use the python's usual slicing notation. For example, to get the second element of each of the deepest lists of `B` in the corresponding tensor, one can simply write"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1PnNsaaL_Ij",
        "outputId": "f4a9c83b-717e-4f08-ac02-278d9c704b1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[2, 5],\n",
              "        [0, 1]])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "B_tensor[:,:,1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CoaVWTtHjN1"
      },
      "source": [
        "#### Tensors can be used in computations element-wise, as long as the dimensions match"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdFQUCmzHijV",
        "outputId": "78db96f2-9a52-4b67-da44-3471c084a312"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 1.7000,  5.4000, 11.1000],\n",
              "         [18.8000, 28.5000, 40.2000]],\n",
              "\n",
              "        [[ 0.0000,  0.0000,  0.0000],\n",
              "         [ 1.7000,  1.7000,  1.7000]]])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "B_tensor + B_tensor ** 2 - 0.3 * B_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5Ndj7QtUitM"
      },
      "source": [
        "**Exercise.** With the help of PyTorch documentation online, find the square root of $B^3$. All operations are applied element-wise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvAOPJRljowJ",
        "outputId": "16d267f9-1d61-4bf8-d85f-00e68e53d414"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[1, 2, 3],\n",
              "         [4, 5, 6]],\n",
              "\n",
              "        [[0, 0, 0],\n",
              "         [1, 1, 1]]])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "B_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmGpu-sjZrNQ",
        "outputId": "4774b7e7-4b9e-49b6-afcd-d0599e5a8308"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[1., 2., 3.],\n",
              "         [4., 5., 6.]],\n",
              "\n",
              "        [[0., 0., 0.],\n",
              "         [1., 1., 1.]]])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# your solution\n",
        "(B_tensor **3) ** (1/3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaFKUKGsGnmC"
      },
      "source": [
        "#### Tensors can be either trainable or non-trainable\n",
        "\n",
        "The trainable tensors are the ones that have `require_gradient` set to `True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZrpvNetHHks",
        "outputId": "20bfd26a-079b-48b9-ec73-f60fad59f373"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A_tensor is trainable: False\n",
            "A_trainable is trainable: True\n"
          ]
        }
      ],
      "source": [
        "A_trainable = torch.tensor(A, requires_grad=True)\n",
        "\n",
        "print(f\"A_tensor is trainable: {A_tensor.requires_grad}\")\n",
        "print(f\"A_trainable is trainable: {A_trainable.requires_grad}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6JZJgF0yR5o",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Differentiable Functions and Why They Are So Special\n",
        "\n",
        "The entire world is now interested in a particular sub-class of functions, called *differentiable functions*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11getKqlvbJE"
      },
      "source": [
        "A differentiable functions is a function $f$ such that $f$ is differentiable with respect to its parameters. Here is an example of a differentiable function $f$ taking $x$ as input and multiplying it by a parameter $p$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "958evPdf2rA7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0fCXWtzv6et"
      },
      "outputs": [],
      "source": [
        "p = torch.tensor([ 1.0 ], requires_grad=True)\n",
        "\n",
        "def f(x):\n",
        "  global p\n",
        "  return p * x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEV5JmM5v8IG"
      },
      "source": [
        "Apart from forcing software engineers to dust off their high-school calculus knowledge, what are these differentiable programs actually good for? Why has the entire software engineering and data science world gone crazy over them?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWwo3vb00uaO"
      },
      "source": [
        "We won't keep you in suspense, here's the \"secret\":\n",
        "\n",
        "> Given input-output data, differentiable functions can be taught, through gradient descent, to use the right parameters, that is paramteres that extremise a (loss) function.\n",
        "\n",
        "So, in the example of `f` above, we could train the function to learn the best value of `p`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfHkDnBA1km6"
      },
      "source": [
        "The method enabling this is known as gradient descent training. This has been covered well in many lectures and online resources. If you need a quick refresher, have a look through at the corresponding videos from the Computational Thinking course, available [here](https://disco.ethz.ch/courses/hs24/coti/). The short version is that we can update the parameter `p` using the update:\n",
        "\n",
        "$p_{new} = p - ∇_pf(x)$\n",
        "\n",
        "where $∇_p f(x)$ is the gradient of `f` with respect to `p` for the data `x`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4gmO1GKilew"
      },
      "source": [
        "### Gradient Computation in PyTorch\n",
        "At the helm of gradient-descent training in PyTorch is the `autograd` module. `torch.autograd` is PyTorch's automatic differentiation engine that powers gradient-descent training.\n",
        "\n",
        "Suppose you take some trainable tensor $x$ and pass it through $f$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovOmObj-aCnr",
        "outputId": "cc59339d-d4d1-480a-a2f0-5d376837596d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([5.1000], grad_fn=<MulBackward0>)"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.tensor([ 5.1 ], requires_grad=True)\n",
        "output = f(x)\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcDXcjV3aTK5"
      },
      "source": [
        "Notice that `output` now has an additional field, `grad_fn`, that was set by the `autograd` system to keep track of what operations have been performed on `x` to arrive at output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKlQaJ9Ialbh"
      },
      "source": [
        "Now, given some expected output for $f(x)$, say $1$, `autograd` allows you to compute an indication of how `p` needs to be changed in order for $f(x)$ to eventually yield the correct outputs. This is done in a process called *backward pass*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6nNrPt4bLbh"
      },
      "outputs": [],
      "source": [
        "# define the output we expect. I.e. the output of f(x) with the correct p\n",
        "expected_output = torch.tensor([1.0])\n",
        "# compute the loss (how \"bad\" that output is for the current value of p)\n",
        "loss = (output - expected_output) ** 2\n",
        "# compute the gradient of f w.r.t. p\n",
        "loss.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6I-wj_QbV83"
      },
      "source": [
        "This indication can then by inspected by asking `p` what its gradient is by reading `p.grad`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YanreQybVVa",
        "outputId": "61de84ac-38fb-42f8-baba-94ce117769c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([41.8200])"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "p.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vNxdfQgcNcq"
      },
      "source": [
        "This indication can be interpreted as\n",
        "\n",
        "> Decreasing `p` by some small $\\epsilon$ will decrease the loss by $41.82\\epsilon$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVLVUzhpjQOS"
      },
      "source": [
        "And, as you already know, leveraging the negation of gradient as the indication of the direction in which one should modify the parameters in order to descent towards lower values of the loss, gradient descent training is simply the routine under which one iteratively computes and then applies the gradient of the loss function with respect to parameters of the computation to minimise the loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s93DpbnznITx"
      },
      "source": [
        "**Exercise.** Fill in the code below to compute the gradients for `p` equal to `0.75`, `0.5`, `0.25`, `0.20` and `0.10`. What do you observe?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoWVAgtCnlT6",
        "outputId": "5845bb27-eec6-4901-d642-77f9c93bb61e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For p = 0.75, the gradient is tensor([28.8150])\n",
            "For p = 0.50, the gradient is tensor([15.8100])\n",
            "For p = 0.25, the gradient is tensor([2.8050])\n",
            "For p = 0.20, the gradient is tensor([0.2040])\n",
            "For p = 0.10, the gradient is tensor([-4.9980])\n"
          ]
        }
      ],
      "source": [
        "# case p = 0.75\n",
        "p = torch.tensor([ 0.75 ], requires_grad=True)\n",
        "# you want something gradienty here - remember the backwards call :)\n",
        "loss = (f(x) - expected_output) ** 2\n",
        "loss.backward()\n",
        "\n",
        "print(f\"For p = 0.75, the gradient is {p.grad}\")\n",
        "\n",
        "\n",
        "# case p = 0.50\n",
        "p = torch.tensor([ 0.50 ], requires_grad=True)\n",
        "loss = (f(x) - expected_output) ** 2\n",
        "loss.backward()\n",
        "\n",
        "print(f\"For p = 0.50, the gradient is {p.grad}\")\n",
        "\n",
        "\n",
        "# case p = 0.25\n",
        "p = torch.tensor([ 0.25 ], requires_grad=True)\n",
        "loss = (f(x) - expected_output) ** 2\n",
        "loss.backward()\n",
        "\n",
        "print(f\"For p = 0.25, the gradient is {p.grad}\")\n",
        "\n",
        "\n",
        "# case p = 0.20\n",
        "p = torch.tensor([ 0.20 ], requires_grad=True)\n",
        "loss = (f(x) - expected_output) ** 2\n",
        "loss.backward()\n",
        "\n",
        "print(f\"For p = 0.20, the gradient is {p.grad}\")\n",
        "\n",
        "\n",
        "# case p = 0.10\n",
        "p = torch.tensor([ 0.10 ], requires_grad=True)\n",
        "loss = (f(x) - expected_output) ** 2\n",
        "loss.backward()\n",
        "\n",
        "print(f\"For p = 0.10, the gradient is {p.grad}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33PSV9YLnxy4"
      },
      "source": [
        "### A Basic Training Loop\n",
        "As hinted on by the above example, modifying the parameters of a differentiable program in the direction opposite to its gradient (i.e. in the direction in which the loss decreases most rapidly) generally guides the differentiable programs towards a minimum in the loss.\n",
        "\n",
        "This process can be repeated iteratively, to form what is called a *training loop*. A typical training procedure of a differentiable program looks as follows:\n",
        "\n",
        "\n",
        "1. Initialise the parameters of the differentiable program according to an appropriate scheme.\n",
        "2. Take the inputs provided and perform a *forward pass* -- apply the program to the inputs.\n",
        "3. Compute the loss between the expected outputs and the actual outputs of the program.\n",
        "4. Compute the gradient of the loss with respect to the program's parameters.\n",
        "5. Scale the gradients by the desired pace of descent -- *the learning rate* -- and update the parameters accordingly.\n",
        "6. Repeat step 2 to 5 for a number of rounds or until the loss is sufficently small.\n",
        "\n",
        "\n",
        "You already possess all the basic ingredients necessary to implement such a training procedure yourself. Let's do that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zTTMnCiUOph"
      },
      "source": [
        "#### Training loop without PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3G-phKdX0d0"
      },
      "source": [
        "To get a feeling for how we can optimize the parameter `p` to some data we will start without the help PyTorch gives. We will still use PyTorch for tensor operations, but this could also be done using NumPy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcgtJtJIUUHV"
      },
      "outputs": [],
      "source": [
        "def f(x, p):\n",
        "    return x * p\n",
        "\n",
        "def grad_f(x, p):\n",
        "    return x\n",
        "\n",
        "def loss_function(x, p, y):\n",
        "    y_hat = f(x, p)\n",
        "    errors = y_hat - y\n",
        "    loss = torch.mean(errors**2)\n",
        "    return loss\n",
        "\n",
        "def grad_loss_function(x, p, y):\n",
        "    y_hat = f(x, p)\n",
        "    grad_y_hat = grad_f(x, p)\n",
        "    errors = y_hat - y\n",
        "    grad_errors = grad_y_hat\n",
        "    grad_loss = torch.mean( 2*errors*grad_errors )\n",
        "    return grad_loss\n",
        "\n",
        "\n",
        "def training_loop(x, y, learning_rate: float = 0.1, number_of_iterations: int = 100):\n",
        "    p = 0.9\n",
        "\n",
        "    for iteration in range(number_of_iterations):\n",
        "        # compute the gradient\n",
        "        grad_loss = grad_loss_function(x, p, y)\n",
        "\n",
        "        # log the result every 10th iteration\n",
        "        if iteration%10 == 0:\n",
        "            print(f\"iteration: {iteration:3}, p: {p:5.3f}, gradient: {grad_loss}\")\n",
        "\n",
        "        # update p\n",
        "        p = p - learning_rate * grad_loss\n",
        "\n",
        "    return p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2rqqDO8axUt",
        "outputId": "d78d6eeb-8248-46c6-9aa1-97ab0d700498"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration:   0, p: 0.900, gradient: 0.29059097170829773\n",
            "iteration:  10, p: 0.203, gradient: 0.0013610561145469546\n",
            "iteration:  20, p: 0.200, gradient: 6.37252242086106e-06\n",
            "iteration:  30, p: 0.200, gradient: 3.181057905976559e-08\n",
            "iteration:  40, p: 0.200, gradient: 4.3690815410002415e-09\n",
            "iteration:  50, p: 0.200, gradient: 4.3690815410002415e-09\n",
            "iteration:  60, p: 0.200, gradient: 4.3690815410002415e-09\n",
            "iteration:  70, p: 0.200, gradient: 4.3690815410002415e-09\n",
            "iteration:  80, p: 0.200, gradient: 4.3690815410002415e-09\n",
            "iteration:  90, p: 0.200, gradient: 4.3690815410002415e-09\n",
            "The true value is 0.2, the value learned by gradient descent is 0.20000001788139343\n"
          ]
        }
      ],
      "source": [
        "p_true = 0.2 # the parameter value of p to be learned\n",
        "datapoint_count = 10 # the number of datapoints to use for training in every iteration of the training loop\n",
        "\n",
        "# Generate some artifical data. (datapoint_count, ) gives a one-dimensional array of length datapoint_count.\n",
        "x = torch.rand(datapoint_count, requires_grad=False)\n",
        "y = x * p_true\n",
        "\n",
        "p_found = training_loop(x, y, learning_rate=1)\n",
        "print(f\"The true value is {p_true}, the value learned by gradient descent is {p_found}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1YSJDPigASP"
      },
      "source": [
        "**Exercise.** Play around with the number of iterations and learning rate. What happens if the learning rate is very high (>5) or very small (<1e-6)?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YO7Q4H3OUUrG"
      },
      "source": [
        "#### Training loop with PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nYngzhdoCRt"
      },
      "source": [
        "We need to define the function using PyTorch's syntax.\n",
        "A function in PyTorch is a `class` with a `forward` function.\n",
        "Forward defines what should happen when you say `f(x)` and the building blocks\n",
        "are the functions in `torch.nn`, e.g., `nn.Linear`. The next section will look at this in details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dn1YONH9lr5r"
      },
      "outputs": [],
      "source": [
        "\n",
        "class f_function(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Make a linear function (y=ax+b) from input to output.\n",
        "        # bias=False means b=0, so we get y = p*x\n",
        "        self.p = nn.Linear(1, 1, bias=False)\n",
        "\n",
        "        # set p to a fixed value\n",
        "        # the torch.no_grad() removes gradient computations, so p.grad does not change\n",
        "        with torch.no_grad():\n",
        "            self.p.weight[0][0] = 0.9\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.p(x)\n",
        "\n",
        "def train_f(x, y, learning_rate: float = 0.1, number_of_iterations: int = 100):\n",
        "    # define the function\n",
        "    f = f_function()\n",
        "    # define the loss function as the mean squared loss\n",
        "    loss_fn = nn.MSELoss()\n",
        "    # define the optimizer as classic gradient decent\n",
        "    optimizer = torch.optim.SGD(f.parameters(), lr=learning_rate)\n",
        "\n",
        "    for iteration in range(number_of_iterations):\n",
        "        # perform a \"forward pass\" (apply f to x)\n",
        "        y_hat = f(x)\n",
        "\n",
        "        # compute the loss\n",
        "        loss = loss_fn(y_hat, y)\n",
        "\n",
        "        # zero the gradients computed in the previous step\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # calculate the gradients of the parameters of the net\n",
        "        loss.backward()\n",
        "\n",
        "        # use the gradients to update the weights of the network\n",
        "        optimizer.step()\n",
        "\n",
        "    # return p\n",
        "    return f.p.weight[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mmw4UKFl5Kh"
      },
      "source": [
        "**Exercise.** Complete the code below and compare the results to the implementation without PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xA81E0Ll1N5",
        "outputId": "a7bf1c1e-78ce-4eb6-f928-0ecab0429602"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The true value is 0.2, the value learned by gradient descent is 0.20000001788139343\n"
          ]
        }
      ],
      "source": [
        "# pytorch needs the shape to be an array of shape (datapoint_count, 1)\n",
        "x_pytorch = x.reshape( datapoint_count, 1 )\n",
        "y_pytorch = y.reshape( datapoint_count, 1 )\n",
        "\n",
        "p_found = train_f(x_pytorch, y_pytorch, learning_rate=1)\n",
        "print(f\"The true value is {p_true}, the value learned by gradient descent is {p_found}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PXg-iPLsNLV"
      },
      "source": [
        "## Introducing Neural Networks\n",
        "Neural networks are a particular class of differentiable programs, for which it has been theoretically proven that they can learn to approximate an arbitrary integrable function arbitrarily well, as long as they are given enough *representational power*.\n",
        "\n",
        "*Here is where the deep learning black magic begins.*\n",
        "\n",
        "Classical programs consist of a sequence of specific operations such as addition or conditional value assignment. Neural networks are differentiable programs that consist of a sequence of amenable elementary building blocks, traditionally referred to as *layers*, that can ultimately perform a wide variety of operations. The \"bigger\" these layers are, the more complex behaviour they can learn to exhibit.\n",
        "\n",
        "There exists several popular types of neural network building blocks, including the trainable *linear layer*, or the non-trainable *activation*, *softmax*, and *dropout layers*, to name but a few. The combination of a linear layer and an activation layer is sometimes referred to as *dense layer* and is the basic building block of a *deep neural network*.\n",
        "\n",
        "The amount of *representational power* network has is determined by the sizes of its trainable layers. Linear layers have a \"width\" (the number of constituent neurons). The wider the layer, the more fine-grained operation it is capable of representing. Whether it can learn to represent this operation is, however, an entirely different question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SlXTgnk52rA_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSgEwtUzQSEq"
      },
      "source": [
        "### Constructing Neural Networks\n",
        "Without further ado, let use these building blocks to form neural networks.\n",
        "\n",
        "Knowing the format of the operation of individual layers, you could go ahead and implement them manually. To avoid uncanny detail, we will instead use the ready-made implementations of these layers from the `torch.nn` module.\n",
        "\n",
        "In general, the [documentation](https://pytorch.org/docs/stable/nn.html) of the `torch.nn` module is what you want to turn to to understand a new layer type.\n",
        "\n",
        "We walk you through creating instances of various layer types in the code below. We directly use the instances to operate input data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CX5xxzB1vXwi"
      },
      "source": [
        "#### Linear Layers\n",
        "\n",
        "Let us begin with the most basic layer in deep learning, the Linear layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dUDE6jjQRQW",
        "outputId": "67a2f927-cead-4257-e2b8-c88b62aaf07c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-1.3033,  0.1348,  1.5318, -0.0683,  0.8872], grad_fn=<ViewBackward0>)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# construct a linear layer that takes a tensor of size (3,) and produces a\n",
        "#  tensor of size (5,)\n",
        "linear_layer = nn.Linear(3, 5)\n",
        "\n",
        "# pass [1, 2, 3] through the layer\n",
        "example_input = torch.tensor([ 1, 2, 3 ], dtype=torch.float)\n",
        "linear_layer(example_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqrHdPdHvehw"
      },
      "source": [
        "Notice that as promised in the call to `nn.Linear(3, 5)`, the output tensor has 5 entries. Its output values are the result of an internal state (the layer *weights*) that has been initialised with random weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SQI0UMFwxj2"
      },
      "source": [
        "#### Activation Layers\n",
        "\n",
        "Several types of activation layers exist, most notably the logistic sigmoid, rectified linear unit (ReLU), and the hyperbolic tangent. Each of these has a layer in `torch.nn`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7tTJxVGu1wS",
        "outputId": "47656748-7023-4d91-9b26-756102f886ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 0., 1., 2., 3.])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# construct a ReLU layer\n",
        "relu_layer = nn.ReLU()\n",
        "\n",
        "# pass [ -3, -2, -1, 0, +1, +2, +3 ] through the ReLU layer\n",
        "example_input = torch.tensor([ -3, -2, -1, 0, +1, +2, +3 ], dtype=torch.float)\n",
        "relu_layer(example_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "st3FTSinzry1",
        "outputId": "80374152-2cec-47e4-8f57-4cb598ab10c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.0474, 0.1192, 0.2689, 0.5000, 0.7311, 0.8808, 0.9526])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# construct a sigmoid layer\n",
        "sigmoid_layer = nn.Sigmoid()\n",
        "\n",
        "# pass [ -3, -2, -1, 0, +1, +2, +3 ] through the sigmoid layer\n",
        "sigmoid_layer(example_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHPmBuUdz4xL",
        "outputId": "be127135-cc99-41d2-874b-194491f1d25d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-0.9951, -0.9640, -0.7616,  0.0000,  0.7616,  0.9640,  0.9951])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# construct a tanh layer\n",
        "tanh_layer = torch.nn.Tanh()\n",
        "\n",
        "# pass [ -3, -2, -1, 0, +1, +2, +3 ] through the tanh layer\n",
        "tanh_layer(example_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvrPbIhrz7J1"
      },
      "source": [
        "As you can see, going from minus infinity towards infinity around 0, the ReLU transits from constant 0 to linear behaviour at 0, the logistic sigmoid proceeds to climb from 0 towards 1, and tanh climbs from -1 towards +1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLC-6JYK0Oji"
      },
      "source": [
        "#### Dropout Layers\n",
        "\n",
        "It is sometimes to the advantage of model training to \"drop out\" some of the incoming values at random. To this end, `torch.nn` provides the `Dropout` layer, which can be parameterised at construction with the probability of an input value being dropped out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PLHSgNbz54s",
        "outputId": "b16a0218-df2f-4fa9-d1f4-8cc6c5a974fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-0., -0., -0., 0., 0., 0., 6.])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# construct a dropout layer with probability 0.5\n",
        "dropout_layer = torch.nn.Dropout(p=0.5)\n",
        "\n",
        "# pass [ -3, -2, -1, 0, +1, +2, +3 ] through the dropout layer\n",
        "dropout_layer(example_input)\n",
        "\n",
        "# with p=0.5, roughly half of the inputs should be dropped out on average,\n",
        "#  and the remaining outputs are scaled up by 1/(1-p) == 2\n",
        "# run this snippet multiple times to observe the effects of random dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uri42H-24L80"
      },
      "source": [
        "#### Softmax\n",
        "The softmax function is used when we want to interpret an $n$-dimensional vector of real values as probabilities associated with $n$ discrete options. It helps determine the likelihood of each option possessing a certain property.\n",
        "\n",
        "The softmax layer transforms the input $n$-dimensional vector into another $n$-dimensional vector, where all values lie between $0$ and $1$ and sum to $1$.\n",
        "\n",
        "The larger an entry in the input vector is relative to the others, the closer its corresponding output value is to $1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lv4tVwYK3WZc",
        "outputId": "e1f70099-691b-4482-fb76-4ed5cc01ad5c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.0016, 0.0043, 0.0116, 0.0315, 0.0856, 0.2328, 0.6327])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# construct a softmax layer\n",
        "softmax_layer = torch.nn.Softmax(dim=0)\n",
        "\n",
        "# pass [ -3, -2, -1, 0, +1, +2, +3 ] through the softmax layer\n",
        "softmax_layer(example_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvmT75wPoiYq",
        "outputId": "01e32876-6563-4bad-df9c-59879a8ce314"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1.4013e-44, 2.7465e-43, 5.5211e-42, 1.0000e+00])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# pass [ -1, 2, 5, 100 ] through the softmax layer\n",
        "softmax_layer(torch.tensor([ -1, 2, 5, 100 ], dtype=torch.float))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hF_z6JC4M_V"
      },
      "source": [
        "### Putting the Layers Together -- Implementing a DNN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW_XVRYLuLJG"
      },
      "source": [
        "Neural networks are graphs of layers. In PyTorch, we generally tend to implement our neural networks as classes whose constructors construct the constituent parts of the network, and whose `forward` function passes the data through these parts.\n",
        "\n",
        "Below is an example implementation of a shallow neural network. This network is *shallow* as it contains only one hidden trainable layer (=layer that is not an input or output layer).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8boLlg2HwJhg"
      },
      "outputs": [],
      "source": [
        "class ShallowNeuralNet(nn.Module):\n",
        "    def __init__(self, input_width: int, hidden_layer_width: int, output_width):\n",
        "        super().__init__()\n",
        "        self.hidden_layer = nn.Linear(input_width, hidden_layer_width)\n",
        "        self.hidden_relu = nn.ReLU()\n",
        "        self.output_layer = nn.Linear(hidden_layer_width, output_width)\n",
        "\n",
        "    def forward(self, input):\n",
        "        hidden_trainable_output = self.hidden_layer(input)\n",
        "        hidden_relu_output = self.hidden_relu(hidden_trainable_output)\n",
        "        output = self.output_layer(hidden_relu_output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Le9mkI0Lwz0i"
      },
      "source": [
        "Once the network behaviour has been described in this fashion, we can create an instance of the entire network at once and use it to process data in exactly the same way as we would use layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjQx_ztbwzGG",
        "outputId": "80368569-bfd7-4ec8-9d25-7173a16a38f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-0.1283, -0.1558], grad_fn=<ViewBackward0>)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "shallow_nn_instance = ShallowNeuralNet(5, 10, 2)\n",
        "\n",
        "example_input = torch.ones(5)\n",
        "shallow_nn_instance(example_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxG85zD10uCD"
      },
      "source": [
        "**Exercise.** Complete the forward pass below to arrive at an implementation of a deep ReLU neural networks with layer profile given by a list of integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GIP3GTxM0tZF"
      },
      "outputs": [],
      "source": [
        "class DeepNeuralNet(nn.Module):\n",
        "  def __init__(self, input_width, hidden_layer_profile, output_width, output_activation=None):\n",
        "    super().__init__()\n",
        "    self.layers = nn.ModuleList()\n",
        "\n",
        "    # create the first hidden layer\n",
        "    self.layers.append(nn.Linear(input_width, hidden_layer_profile[0]))\n",
        "    self.layers.append(nn.ReLU())\n",
        "\n",
        "    # create the internal hidden layers\n",
        "    for in_width, out_width in zip(hidden_layer_profile[0:-1], hidden_layer_profile[1:]):\n",
        "      self.layers.append(nn.Linear(in_width, out_width))\n",
        "      self.layers.append(nn.ReLU())\n",
        "\n",
        "    self.layers = nn.Sequential(*self.layers)\n",
        "\n",
        "    # create the output layer\n",
        "    self.output_layer = nn.Linear(hidden_layer_profile[-1], output_width)\n",
        "    self.output_activation = nn.Identity() if not output_activation else output_activation\n",
        "\n",
        "  def forward(self, input):\n",
        "    x = input\n",
        "\n",
        "    # loop through the layers to produce the output of the hidden network\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "\n",
        "    # TODO: produce the output of the network from the intermediate output of the last hidden layer\n",
        "    output_before_activation = self.output_layer(x)\n",
        "\n",
        "    # TODO: engage the optional activation in self.output_activation on the output_before_activation\n",
        "    output = self.output_activation(output_before_activation)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ctV1s9_2lw_"
      },
      "source": [
        "**Exercise.** Test the class for generic deep neural networks below. Does everything work as expected?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApoMITVs2wLd",
        "outputId": "e0c53778-ae8a-4dcc-fbc9-f8b5f601aeb5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.3156], grad_fn=<ViewBackward0>)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# try passing a tensor of random numbers through the network\n",
        "input1 = torch.rand((10,))\n",
        "deep_nn_instance = DeepNeuralNet(input_width=10, hidden_layer_profile=[10, 7, 5], output_width=1)\n",
        "\n",
        "deep_nn_instance(input1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqrY3Z77-TYq"
      },
      "source": [
        "### A Training Loop for Neural Networks\n",
        "In a previous section concerning differentiable functions, we introduced the intuition for using the gradient information due to a choice of loss function to find optimal parameters of a differentiable function.\n",
        "\n",
        "This is exactly what we do for neural networks as well in order to train them to have the behaviour we desire of them.\n",
        "\n",
        "We give code for optimisation of a neural network `net` with particular loss function `loss` on dataset loaded by a `dataloader` below, and comment on it step by step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0GWPK-epiM3Q"
      },
      "outputs": [],
      "source": [
        "# Use the gpu if you are connected to a runtime with one.\n",
        "# This is highly recommended, as it makes computations much faster\n",
        "# (especially later on for the larger models)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def training_loop(\n",
        "        dataloader: torch.utils.data.DataLoader,\n",
        "        net: nn.Module,\n",
        "        loss_fn: nn.Module,\n",
        "        optimiser: torch.optim.Optimizer,\n",
        "        verbosity: int=3,\n",
        "        device = DEVICE):\n",
        "    size = len(dataloader.dataset)\n",
        "    last_print_point = 0\n",
        "    current = 0\n",
        "\n",
        "    acc_loss = 0\n",
        "    acc_count = 0\n",
        "    net.train()\n",
        "    # for every slice (X, y) of the training dataset\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # perform a forward pass to compute the outputs of the net\n",
        "        pred = net(X)\n",
        "\n",
        "        # calculate the loss between the outputs of the net and the desired outputs\n",
        "        loss_val = loss_fn(pred, y)\n",
        "        acc_loss += loss_val.item()\n",
        "        acc_count += 1\n",
        "\n",
        "        # zero the gradients computed in the previous step\n",
        "        optimiser.zero_grad()\n",
        "\n",
        "        # calculate the gradients of the parameters of the net\n",
        "        loss_val.backward()\n",
        "\n",
        "        # use the gradients to update the weights of the network\n",
        "        optimiser.step()\n",
        "\n",
        "        # compute how many datapoints have already been used for training\n",
        "        current = batch * len(X)\n",
        "\n",
        "        # report on the training progress roughly every 10% of the progress\n",
        "        if verbosity >= 3 and (current - last_print_point) / size >= 0.1:\n",
        "            loss_val = loss_val.item()\n",
        "            last_print_point = current\n",
        "            print(f\"loss: {loss_val:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return acc_loss / acc_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o84sL9H2pNR2"
      },
      "source": [
        "We now possess all the tools necessary for constructing simple neural networks and for training them towards some particular behaviour by gradient descent loss minimisation. Let us put these tools to good use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ogK_KsxYZDN"
      },
      "source": [
        "### Learning a Boolean Function\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAPqtEvW3Kzj"
      },
      "source": [
        "Let us consider the problem of learning a random Boolean function $f: \\left\\{ 0,1 \\right\\}^n \\to \\left\\{ 0,1 \\right\\}^n$. It might sound a bit boring at first, but bear with us, as it is a very natural and tractable example for the examination of the representational power of various types of neural networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CVoL-jQFYnDi"
      },
      "outputs": [],
      "source": [
        "def make_binary_array(number: int, length: int) -> list:\n",
        "\treturn [ (number>>k)&1 for k in range(0, length) ]\n",
        "\n",
        "data_x_list = []\n",
        "\n",
        "for i in range(0, 256):\n",
        "  data_x_list.append(make_binary_array(i, 8))\n",
        "\n",
        "data_x = torch.tensor(data_x_list, dtype=torch.float)\n",
        "data_y = torch.randint(low=0, high=2, size=(256, 8)).type(torch.float)\n",
        "# Move data to compute device\n",
        "data_x = data_x.to(DEVICE)\n",
        "data_y = data_y.to(DEVICE)\n",
        "\n",
        "dataset = torch.utils.data.TensorDataset(data_x, data_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FqdC15d3gkg"
      },
      "source": [
        "The above code generates a dataset of $(x, y)$ pairs where $x$ is any $n=8$-bit signal and $y = f(x)$, with $f$ chosen uniformly at random."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTVEVnlMBrCt"
      },
      "source": [
        "Given these examples, can we learn a neural network that performs the function of $f$? Yes! Just run the code below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JAUZSwWBLyd4"
      },
      "outputs": [],
      "source": [
        "def testing_loop(dataloader, net, device = DEVICE):\n",
        "  size = len(dataloader.dataset)\n",
        "  last_print_point = 0\n",
        "  current = 0\n",
        "\n",
        "  acc_correct = 0\n",
        "  acc_count = 0\n",
        "\n",
        "  net.eval()\n",
        "  # for every slice (X, y) of the training dataset\n",
        "  with torch.no_grad():\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # perform a forward pass to compute the outputs of the net\n",
        "        pred = net(X)\n",
        "\n",
        "        # round the predictions (0 - 0.5 towards zero, >0.5 towards one)\n",
        "        pred_rounded = torch.round(pred)\n",
        "\n",
        "        # compute the number of correct entries\n",
        "        acc_correct += torch.count_nonzero(pred_rounded == y).item()\n",
        "        acc_count += y.numel()\n",
        "\n",
        "\n",
        "  return acc_correct / acc_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FgKbUBdWMAm1"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train(dataloader, net, loss_fn, optimiser, epochs, epoch_frequency=50, device=DEVICE, verbosity=3):\n",
        "  least_loss = None\n",
        "  if verbosity < 2:\n",
        "    for t in tqdm(range(epochs), desc=\"Training Progress\", unit=\" epochs\"):\n",
        "      mean_loss = training_loop(dataloader, net, loss_fn, optimiser, verbosity=verbosity)\n",
        "      accuracy = testing_loop(dataloader, net)\n",
        "      if not least_loss or mean_loss < least_loss:\n",
        "        least_loss = mean_loss\n",
        "  else:\n",
        "    for t in range(epochs):\n",
        "        if verbosity >= 3:\n",
        "          print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "\n",
        "        mean_loss = training_loop(dataloader, net, loss_fn, optimiser, verbosity=verbosity)\n",
        "        accuracy = testing_loop(dataloader, net)\n",
        "        if not least_loss or mean_loss < least_loss:\n",
        "          least_loss = mean_loss\n",
        "\n",
        "        if verbosity >= 2 and t%epoch_frequency == 0:\n",
        "          print(f\"Epoch {t:4}: mean loss {mean_loss:.5f}, validation accuracy {accuracy:7.2%}\")\n",
        "        if verbosity >= 3:\n",
        "          print(\"\\n\")\n",
        "\n",
        "  if verbosity >= 1:\n",
        "    print(f\"\\nTraining complete, least loss {least_loss}, final validation accuracy {accuracy:.2%}\")\n",
        "\n",
        "  return least_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qv36ZZ4aXkmA",
        "outputId": "a23e6cfa-a4a3-41dc-b5f3-46182b0fe44c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 400/400 [00:08<00:00, 47.46 epochs/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training complete, least loss 0.0009081615571631119, final validation accuracy 100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "net = DeepNeuralNet(input_width=8, hidden_layer_profile=[256], output_width=8, output_activation=nn.Sigmoid()).to(DEVICE)\n",
        "\n",
        "training_dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True, drop_last=True)\n",
        "loss_fn = nn.BCELoss()\n",
        "optimiser = torch.optim.Adam(net.parameters(), lr=2e-2)\n",
        "\n",
        "least_loss = train(training_dataloader, net, loss_fn, optimiser, epochs=400, verbosity=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkVWsgZU36Bg"
      },
      "source": [
        "Okay, this is encouraging. We are getting a loss in the order of $10^{-3}$ (which is relatively little for mean binary cross-entropy) and 100% accuracy. You will notice that as the training progresses, the loss tends to decrease and the accuracy increases. You will also notice that our neural network has only one hidden layer of 256 neurons. But are all those neurons really necessary?\n",
        "\n",
        "Let's push things to an extremum and consider a network that has exactly one neuron in its hidden layer. In other words, all of the information about the input the output neurons have must be contained in exactly one activated number, and the hidden layer of such a network is an information bottleneck. All other parameters constant, what sort of loss values and accuracies will we be getting under such circumstances?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nIrvnj98dN5",
        "outputId": "f2af3c3a-ccb5-4701-a8f5-928536877432"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 400/400 [00:07<00:00, 53.31 epochs/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training complete, least loss 0.6861061975359917, final validation accuracy 54.64%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "slender_net = DeepNeuralNet(input_width=8, hidden_layer_profile=[ 1 ], output_width=8, output_activation=nn.Sigmoid()).to(DEVICE)\n",
        "\n",
        "training_dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "loss_fn = nn.BCELoss()\n",
        "optimiser = torch.optim.Adam(slender_net.parameters(), lr=2e-2)\n",
        "\n",
        "least_slender_loss = train(training_dataloader, slender_net, loss_fn, optimiser, epochs=400, verbosity=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFLICSAFATyE"
      },
      "source": [
        "We see that with one hidden neuron, we learn to predict the values of $f(x)$ only marginally better than a coin flip, and that this corresponds to a relatively large value of the binary cross-entropy loss.\n",
        "\n",
        "Okay. So there is a number of hidden neurons (256) that is sufficient for learning $f$ with 100% accuracy, and there is a number of hidden neurons (1) that is clearly insufficient to learn anything but some rough indication of the correct output.\n",
        "\n",
        "*   With one hidden neuron, we have starved the network of representational power to the extent that it is only slightly better than tossing a fair coin at predicting $f$.\n",
        "*   With 256 hidden neurons, we have given the network enough representational power to learn $f$. Perhaps even too much.\n",
        "\n",
        "What happens in between these two extrema? And, is there a number of neurons beyond which the network fails to learn $f$ correctly but for which $f$ can still be learned?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0wKjLNG4Yrn"
      },
      "source": [
        "**Exercise.** Find the least number of neurons $w_1$ such that the training of a shallow neural network with $w$ hidden neurons can still learn to execute $f$ with loss of at most $0.001$. Remember that you can adjust the learning rate and the number of epochs to get finer and more resource-efficient training. You can also set `verbosity=1` to avoid long listings of losses, though verbosity of above `1` might help with the investigation of whether the training losses plateau out.\n",
        "\n",
        "You can also play with the number of epochs and learning rate to ensure the model has finished training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATMk4IcY4vqb",
        "outputId": "3a11d6fb-1404-4ced-f915-499486372d91"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 500/500 [00:20<00:00, 23.95 epochs/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training complete, least loss 0.00010075957867560315, final validation accuracy 100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "w_1 = 256\n",
        "least_loss = 1\n",
        "training_dataloader = torch.utils.data.DataLoader(dataset, batch_size=12, shuffle=True)\n",
        "loss_fn = nn.BCELoss()\n",
        "\n",
        "net = DeepNeuralNet(input_width=8, hidden_layer_profile=[ w_1 ], output_width=8, output_activation=nn.Sigmoid()).to(DEVICE)\n",
        "\n",
        "optimiser = torch.optim.Adam(net.parameters(), lr=.01)\n",
        "least_loss = train(training_dataloader, net, loss_fn, optimiser, epochs=500, verbosity=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2XD3gXi5GdP"
      },
      "source": [
        "**Exercise.** It has been theoretically proven that deep neural networks (that is, networks with more than one hidden layer) can learn the same functions as shallow neural nets while using comparatively fewer neurons and trainable weights. Can you find a `w_2`, being a minimal number of neurons sufficient to learn the function $f$ with loss of at most $0.001$, such that the `w_2` neurons can be distributed in multiple hidden layers? The number of layers you end up using is up to you.\n",
        "\n",
        "Hint: Decrease the learning rate and increase the number of epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uT-6dQcE6UQw",
        "outputId": "1f304af0-d719-4915-8e27-387a0d2904ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using 140 neurons\n",
            "Epoch    0: mean loss 0.70234, validation accuracy  51.71%\n",
            "Epoch   50: mean loss 0.19488, validation accuracy  93.80%\n",
            "Epoch  100: mean loss 0.05006, validation accuracy  98.93%\n",
            "Epoch  150: mean loss 0.01019, validation accuracy 100.00%\n",
            "Epoch  200: mean loss 0.00342, validation accuracy 100.00%\n",
            "Epoch  250: mean loss 0.00184, validation accuracy 100.00%\n",
            "Epoch  300: mean loss 0.00112, validation accuracy 100.00%\n",
            "Epoch  350: mean loss 0.00073, validation accuracy 100.00%\n",
            "\n",
            "Training complete, least loss 0.0005023969933972694, final validation accuracy 100.00%\n"
          ]
        }
      ],
      "source": [
        "# for example, distribute over three layers\n",
        "w_2_1 = 70\n",
        "w_2_2 = 70\n",
        "\n",
        "w_2 = w_2_1 + w_2_2\n",
        "print(f\"Using {w_2} neurons\")\n",
        "net = DeepNeuralNet(input_width=8, hidden_layer_profile=[ w_2_1, w_2_2], output_width=8, output_activation=nn.Sigmoid()).to(DEVICE)\n",
        "\n",
        "training_dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "loss_fn = nn.BCELoss()\n",
        "optimiser = torch.optim.Adam(net.parameters(), lr=2e-2)\n",
        "least_loss = train(training_dataloader, net, loss_fn, optimiser, epochs=400, verbosity=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting optuna\n",
            "  Obtaining dependency information for optuna from https://files.pythonhosted.org/packages/28/09/c4d329f7969443cdd4d482048ca406b6f61cda3c8e99ace71feaec7c8734/optuna-4.2.1-py3-none-any.whl.metadata\n",
            "  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Obtaining dependency information for alembic>=1.5.0 from https://files.pythonhosted.org/packages/99/f7/d398fae160568472ddce0b3fde9c4581afc593019a6adc91006a66406991/alembic-1.15.1-py3-none-any.whl.metadata\n",
            "  Downloading alembic-1.15.1-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Obtaining dependency information for colorlog from https://files.pythonhosted.org/packages/e3/51/9b208e85196941db2f0654ad0357ca6388ab3ed67efdbfc799f35d1f83aa/colorlog-6.9.0-py3-none-any.whl.metadata\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in c:\\users\\maxlu\\anaconda3\\lib\\site-packages (from optuna) (1.24.3)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\maxlu\\anaconda3\\lib\\site-packages (from optuna) (23.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\maxlu\\anaconda3\\lib\\site-packages (from optuna) (1.4.39)\n",
            "Requirement already satisfied: tqdm in c:\\users\\maxlu\\anaconda3\\lib\\site-packages (from optuna) (4.65.0)\n",
            "Requirement already satisfied: PyYAML in c:\\users\\maxlu\\anaconda3\\lib\\site-packages (from optuna) (6.0)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Obtaining dependency information for Mako from https://files.pythonhosted.org/packages/cd/83/de0a49e7de540513f53ab5d2e105321dedeb08a8f5850f0208decf4390ec/Mako-1.3.9-py3-none-any.whl.metadata\n",
            "  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in c:\\users\\maxlu\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\maxlu\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (2.0.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\maxlu\\anaconda3\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\maxlu\\anaconda3\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.1)\n",
            "Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n",
            "   ---------------------------------------- 0.0/383.6 kB ? eta -:--:--\n",
            "   --- ------------------------------------ 30.7/383.6 kB 1.3 MB/s eta 0:00:01\n",
            "   ------- ------------------------------- 71.7/383.6 kB 777.7 kB/s eta 0:00:01\n",
            "   ------------ ------------------------- 122.9/383.6 kB 798.9 kB/s eta 0:00:01\n",
            "   ------------------ ------------------- 184.3/383.6 kB 926.0 kB/s eta 0:00:01\n",
            "   --------------------------- ------------ 266.2/383.6 kB 1.1 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 317.4/383.6 kB 1.1 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 348.2/383.6 kB 1.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 383.6/383.6 kB 1.0 MB/s eta 0:00:00\n",
            "Downloading alembic-1.15.1-py3-none-any.whl (231 kB)\n",
            "   ---------------------------------------- 0.0/231.8 kB ? eta -:--:--\n",
            "   ---------- ----------------------------- 61.4/231.8 kB 1.7 MB/s eta 0:00:01\n",
            "   --------------- ------------------------ 92.2/231.8 kB 1.3 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 143.4/231.8 kB 1.2 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 194.6/231.8 kB 1.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 231.8/231.8 kB 1.1 MB/s eta 0:00:00\n",
            "Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n",
            "   ---------------------------------------- 0.0/78.5 kB ? eta -:--:--\n",
            "   -------------------- ------------------- 41.0/78.5 kB 1.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 78.5/78.5 kB 1.5 MB/s eta 0:00:00\n",
            "Installing collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.9 alembic-1.15.1 colorlog-6.9.0 optuna-4.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-03-06 21:48:14,126] A new study created in memory with name: no-name-bc927be4-e03a-4072-acc7-6d154d0529a7\n",
            "C:\\Users\\maxlu\\AppData\\Local\\Temp\\ipykernel_3120\\3983001074.py:16: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-3, 2e-2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch    0: mean loss 0.69828, validation accuracy  53.91%\n",
            "Epoch   50: mean loss 0.36404, validation accuracy  82.42%\n",
            "Epoch  100: mean loss 0.26801, validation accuracy  87.45%\n",
            "Epoch  150: mean loss 0.21267, validation accuracy  90.43%\n",
            "Epoch  200: mean loss 0.19386, validation accuracy  90.97%\n",
            "Epoch  250: mean loss 0.20006, validation accuracy  90.77%\n",
            "Epoch  300: mean loss 0.18806, validation accuracy  90.72%\n",
            "Epoch  350: mean loss 0.19044, validation accuracy  91.26%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-03-06 21:48:23,893] Trial 0 finished with value: 0.18501131374388932 and parameters: {'num_layers': 2, 'w_2_0': 90, 'w_2_1': 50, 'learning_rate': 0.019008407477591343}. Best is trial 0 with value: 0.18501131374388932.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training complete, least loss 0.16501131374388933, final validation accuracy 92.14%\n",
            "Epoch    0: mean loss 0.69633, validation accuracy  52.69%\n",
            "Epoch   50: mean loss 0.29113, validation accuracy  87.99%\n",
            "Epoch  100: mean loss 0.14136, validation accuracy  94.38%\n",
            "Epoch  150: mean loss 0.09593, validation accuracy  95.70%\n",
            "Epoch  200: mean loss 0.07147, validation accuracy  97.02%\n",
            "Epoch  250: mean loss 0.04703, validation accuracy  98.14%\n",
            "Epoch  300: mean loss 0.11830, validation accuracy  96.63%\n",
            "Epoch  350: mean loss 0.02631, validation accuracy  98.97%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-03-06 21:48:35,275] Trial 1 finished with value: 0.054694719118997454 and parameters: {'num_layers': 3, 'w_2_0': 20, 'w_2_1': 90, 'w_2_2': 70, 'learning_rate': 0.012134374358919533}. Best is trial 1 with value: 0.054694719118997454.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training complete, least loss 0.024694719118997455, final validation accuracy 98.97%\n",
            "Epoch    0: mean loss 0.69603, validation accuracy  52.10%\n",
            "Epoch   50: mean loss 0.53756, validation accuracy  74.51%\n",
            "Epoch  100: mean loss 0.37758, validation accuracy  85.16%\n",
            "Epoch  150: mean loss 0.25273, validation accuracy  92.24%\n",
            "Epoch  200: mean loss 0.16273, validation accuracy  96.92%\n",
            "Epoch  250: mean loss 0.09523, validation accuracy  99.37%\n",
            "Epoch  300: mean loss 0.05783, validation accuracy  99.76%\n",
            "Epoch  350: mean loss 0.03180, validation accuracy 100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-03-06 21:48:45,131] Trial 2 finished with value: 0.0395499521214515 and parameters: {'num_layers': 2, 'w_2_0': 20, 'w_2_1': 100, 'learning_rate': 0.004554220074915049}. Best is trial 2 with value: 0.0395499521214515.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training complete, least loss 0.019549952121451497, final validation accuracy 100.00%\n",
            "Epoch    0: mean loss 0.69344, validation accuracy  50.78%\n",
            "Epoch   50: mean loss 0.55535, validation accuracy  71.78%\n",
            "Epoch  100: mean loss 0.41647, validation accuracy  82.37%\n",
            "Epoch  150: mean loss 0.28920, validation accuracy  89.11%\n",
            "Epoch  200: mean loss 0.19103, validation accuracy  94.82%\n",
            "Epoch  250: mean loss 0.11922, validation accuracy  97.71%\n",
            "Epoch  300: mean loss 0.07152, validation accuracy  99.07%\n",
            "Epoch  350: mean loss 0.03871, validation accuracy  99.80%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-03-06 21:48:55,743] Trial 3 finished with value: 0.05209262643009424 and parameters: {'num_layers': 3, 'w_2_0': 80, 'w_2_1': 20, 'w_2_2': 80, 'learning_rate': 0.0022964234608049165}. Best is trial 2 with value: 0.0395499521214515.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training complete, least loss 0.022092626430094242, final validation accuracy 99.95%\n",
            "Epoch    0: mean loss 0.69433, validation accuracy  52.64%\n",
            "Epoch   50: mean loss 0.40747, validation accuracy  82.03%\n",
            "Epoch  100: mean loss 0.06886, validation accuracy  99.27%\n",
            "Epoch  150: mean loss 0.00618, validation accuracy 100.00%\n",
            "Epoch  200: mean loss 0.00180, validation accuracy 100.00%\n",
            "Epoch  250: mean loss 0.00082, validation accuracy 100.00%\n",
            "Epoch  300: mean loss 0.00045, validation accuracy 100.00%\n",
            "Epoch  350: mean loss 0.00028, validation accuracy 100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-03-06 21:49:08,220] Trial 4 finished with value: 0.04018307975551579 and parameters: {'num_layers': 4, 'w_2_0': 20, 'w_2_1': 100, 'w_2_2': 40, 'w_2_3': 100, 'learning_rate': 0.003647097775124615}. Best is trial 2 with value: 0.0395499521214515.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training complete, least loss 0.00018307975551579148, final validation accuracy 100.00%\n",
            "Epoch    0: mean loss 0.69620, validation accuracy  52.44%\n",
            "Epoch   50: mean loss 0.48253, validation accuracy  76.32%\n",
            "Epoch  100: mean loss 0.32524, validation accuracy  86.04%\n",
            "Epoch  150: mean loss 0.25816, validation accuracy  89.40%\n",
            "Epoch  200: mean loss 0.20515, validation accuracy  91.94%\n",
            "Epoch  250: mean loss 0.25420, validation accuracy  89.06%\n",
            "Epoch  300: mean loss 0.18367, validation accuracy  92.63%\n",
            "Epoch  350: mean loss 0.10260, validation accuracy  96.19%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-03-06 21:49:19,467] Trial 5 finished with value: 0.1267649258673191 and parameters: {'num_layers': 3, 'w_2_0': 90, 'w_2_1': 30, 'w_2_2': 70, 'learning_rate': 0.0076270076472018}. Best is trial 2 with value: 0.0395499521214515.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training complete, least loss 0.09676492586731911, final validation accuracy 96.09%\n",
            "Epoch    0: mean loss 0.69591, validation accuracy  54.83%\n",
            "Epoch   50: mean loss 0.39366, validation accuracy  84.57%\n",
            "Epoch  100: mean loss 0.14927, validation accuracy  98.24%\n",
            "Epoch  150: mean loss 0.04363, validation accuracy 100.00%\n",
            "Epoch  200: mean loss 0.01598, validation accuracy 100.00%\n",
            "Epoch  250: mean loss 0.00732, validation accuracy 100.00%\n",
            "Epoch  300: mean loss 0.00403, validation accuracy 100.00%\n",
            "Epoch  350: mean loss 0.00245, validation accuracy 100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-03-06 21:49:29,086] Trial 6 finished with value: 0.021602474483661354 and parameters: {'num_layers': 2, 'w_2_0': 90, 'w_2_1': 70, 'learning_rate': 0.004269115281601924}. Best is trial 6 with value: 0.021602474483661354.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training complete, least loss 0.0016024744836613536, final validation accuracy 100.00%\n",
            "Epoch    0: mean loss 0.69418, validation accuracy  50.20%\n",
            "Epoch   50: mean loss 0.58508, validation accuracy  68.80%\n",
            "Epoch  100: mean loss 0.42184, validation accuracy  80.18%\n",
            "Epoch  150: mean loss 0.24547, validation accuracy  91.21%\n",
            "Epoch  200: mean loss 0.10578, validation accuracy  98.24%\n",
            "Epoch  250: mean loss 0.03505, validation accuracy 100.00%\n",
            "Epoch  300: mean loss 0.01455, validation accuracy 100.00%\n",
            "Epoch  350: mean loss 0.00596, validation accuracy 100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-03-06 21:49:41,786] Trial 7 finished with value: 0.04317460019141436 and parameters: {'num_layers': 4, 'w_2_0': 40, 'w_2_1': 60, 'w_2_2': 70, 'w_2_3': 90, 'learning_rate': 0.0011283263051792146}. Best is trial 6 with value: 0.021602474483661354.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training complete, least loss 0.0031746001914143562, final validation accuracy 100.00%\n",
            "Epoch    0: mean loss 0.69584, validation accuracy  51.66%\n",
            "Epoch   50: mean loss 0.17576, validation accuracy  94.63%\n",
            "Epoch  100: mean loss 0.00167, validation accuracy 100.00%\n",
            "Epoch  150: mean loss 0.00047, validation accuracy 100.00%\n",
            "Epoch  200: mean loss 0.00022, validation accuracy 100.00%\n",
            "Epoch  250: mean loss 0.00013, validation accuracy 100.00%\n",
            "Epoch  300: mean loss 0.00008, validation accuracy 100.00%\n",
            "Epoch  350: mean loss 0.00005, validation accuracy 100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-03-06 21:49:54,920] Trial 8 finished with value: 0.04003866743577419 and parameters: {'num_layers': 4, 'w_2_0': 40, 'w_2_1': 50, 'w_2_2': 70, 'w_2_3': 70, 'learning_rate': 0.011074801590463583}. Best is trial 6 with value: 0.021602474483661354.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training complete, least loss 3.866743577418674e-05, final validation accuracy 100.00%\n",
            "Epoch    0: mean loss 0.69460, validation accuracy  52.44%\n",
            "Epoch   50: mean loss 0.36294, validation accuracy  81.10%\n",
            "Epoch  100: mean loss 0.13939, validation accuracy  95.12%\n",
            "Epoch  150: mean loss 0.08281, validation accuracy  98.10%\n",
            "Epoch  200: mean loss 0.00374, validation accuracy 100.00%\n",
            "Epoch  250: mean loss 0.00087, validation accuracy 100.00%\n",
            "Epoch  300: mean loss 0.00039, validation accuracy 100.00%\n",
            "Epoch  350: mean loss 0.00021, validation accuracy 100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-03-06 21:50:07,905] Trial 9 finished with value: 0.04012978927588847 and parameters: {'num_layers': 4, 'w_2_0': 40, 'w_2_1': 40, 'w_2_2': 80, 'w_2_3': 70, 'learning_rate': 0.01584511291837294}. Best is trial 6 with value: 0.021602474483661354.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training complete, least loss 0.00012978927588847, final validation accuracy 100.00%\n",
            "Epoch    0: mean loss 0.69421, validation accuracy  52.59%\n",
            "Epoch   50: mean loss 0.56696, validation accuracy  71.73%\n",
            "Epoch  100: mean loss 0.44859, validation accuracy  80.81%\n",
            "Epoch  150: mean loss 0.34274, validation accuracy  88.04%\n",
            "Epoch  200: mean loss 0.25038, validation accuracy  93.21%\n",
            "Epoch  250: mean loss 0.17511, validation accuracy  96.97%\n",
            "Epoch  300: mean loss 0.11825, validation accuracy  98.88%\n",
            "Epoch  350: mean loss 0.07752, validation accuracy  99.80%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-03-06 21:50:17,905] Trial 10 finished with value: 0.07208076350390911 and parameters: {'num_layers': 2, 'w_2_0': 70, 'w_2_1': 80, 'learning_rate': 0.0017058668482301976}. Best is trial 6 with value: 0.021602474483661354.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training complete, least loss 0.05208076350390911, final validation accuracy 100.00%\n",
            "Epoch    0: mean loss 0.69696, validation accuracy  54.69%\n",
            "Epoch   50: mean loss 0.30007, validation accuracy  91.41%\n",
            "Epoch  100: mean loss 0.05590, validation accuracy  99.90%\n",
            "Epoch  150: mean loss 0.01287, validation accuracy 100.00%\n",
            "Epoch  200: mean loss 0.00519, validation accuracy 100.00%\n",
            "Epoch  250: mean loss 0.00264, validation accuracy 100.00%\n",
            "Epoch  300: mean loss 0.00154, validation accuracy 100.00%\n",
            "Epoch  350: mean loss 0.00098, validation accuracy 100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-03-06 21:50:27,468] Trial 11 finished with value: 0.020655536226986442 and parameters: {'num_layers': 2, 'w_2_0': 100, 'w_2_1': 80, 'learning_rate': 0.004960313603103592}. Best is trial 11 with value: 0.020655536226986442.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training complete, least loss 0.0006555362269864418, final validation accuracy 100.00%\n",
            "Epoch    0: mean loss 0.69668, validation accuracy  53.86%\n",
            "Epoch   50: mean loss 0.38538, validation accuracy  86.04%\n",
            "Epoch  100: mean loss 0.13124, validation accuracy  98.00%\n",
            "Epoch  150: mean loss 0.03809, validation accuracy 100.00%\n",
            "Epoch  200: mean loss 0.01306, validation accuracy 100.00%\n",
            "Epoch  250: mean loss 0.00598, validation accuracy 100.00%\n",
            "Epoch  300: mean loss 0.00331, validation accuracy 100.00%\n",
            "Epoch  350: mean loss 0.00203, validation accuracy 100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[W 2025-03-06 21:50:38,114] Trial 12 failed with parameters: {'num_layers': 2, 'w_2_0': 100, 'w_2_1': 70, 'learning_rate': 0.004889865771157186} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\maxlu\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"C:\\Users\\maxlu\\AppData\\Local\\Temp\\ipykernel_3120\\3983001074.py\", line 23, in objective\n",
            "    loss = train(training_dataloader, net, loss_fn, optimiser, epochs=400, verbosity=2)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\maxlu\\AppData\\Local\\Temp\\ipykernel_3120\\1165620050.py\", line 16, in train\n",
            "    mean_loss = training_loop(dataloader, net, loss_fn, optimiser, verbosity=verbosity)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\maxlu\\AppData\\Local\\Temp\\ipykernel_3120\\2060534233.py\", line 26, in training_loop\n",
            "    pred = net(X)\n",
            "           ^^^^^^\n",
            "  File \"c:\\Users\\maxlu\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\maxlu\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\maxlu\\AppData\\Local\\Temp\\ipykernel_3120\\3172420860.py\", line 26, in forward\n",
            "    x = layer(x)\n",
            "        ^^^^^^^^\n",
            "  File \"c:\\Users\\maxlu\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\maxlu\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\maxlu\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 125, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "[W 2025-03-06 21:50:38,116] Trial 12 failed with value None.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Run Optuna study\u001b[39;00m\n\u001b[0;32m     30\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(objective, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)  \u001b[38;5;66;03m# Try 50 different configurations\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Get the best trial\u001b[39;00m\n\u001b[0;32m     34\u001b[0m best_trial \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\n",
            "File \u001b[1;32mc:\\Users\\maxlu\\anaconda3\\Lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     _optimize(\n\u001b[0;32m    476\u001b[0m         study\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    477\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m    478\u001b[0m         n_trials\u001b[38;5;241m=\u001b[39mn_trials,\n\u001b[0;32m    479\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    480\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    481\u001b[0m         catch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[0;32m    482\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    483\u001b[0m         gc_after_trial\u001b[38;5;241m=\u001b[39mgc_after_trial,\n\u001b[0;32m    484\u001b[0m         show_progress_bar\u001b[38;5;241m=\u001b[39mshow_progress_bar,\n\u001b[0;32m    485\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\maxlu\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         _optimize_sequential(\n\u001b[0;32m     64\u001b[0m             study,\n\u001b[0;32m     65\u001b[0m             func,\n\u001b[0;32m     66\u001b[0m             n_trials,\n\u001b[0;32m     67\u001b[0m             timeout,\n\u001b[0;32m     68\u001b[0m             catch,\n\u001b[0;32m     69\u001b[0m             callbacks,\n\u001b[0;32m     70\u001b[0m             gc_after_trial,\n\u001b[0;32m     71\u001b[0m             reseed_sampler_rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     72\u001b[0m             time_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     73\u001b[0m             progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[0;32m     74\u001b[0m         )\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\maxlu\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
            "File \u001b[1;32mc:\\Users\\maxlu\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
            "File \u001b[1;32mc:\\Users\\maxlu\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m func(trial)\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
            "Cell \u001b[1;32mIn[16], line 23\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     20\u001b[0m optimiser \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Train and get the loss\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m loss \u001b[38;5;241m=\u001b[39m train(training_dataloader, net, loss_fn, optimiser, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m, verbosity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     25\u001b[0m w_sum \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(num_layers) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss \u001b[38;5;241m+\u001b[39m w_sum\n",
            "Cell \u001b[1;32mIn[4], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, net, loss_fn, optimiser, epochs, epoch_frequency, device, verbosity)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbosity \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m     14\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m mean_loss \u001b[38;5;241m=\u001b[39m training_loop(dataloader, net, loss_fn, optimiser, verbosity\u001b[38;5;241m=\u001b[39mverbosity)\n\u001b[0;32m     17\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m testing_loop(dataloader, net)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m least_loss \u001b[38;5;129;01mor\u001b[39;00m mean_loss \u001b[38;5;241m<\u001b[39m least_loss:\n",
            "Cell \u001b[1;32mIn[3], line 26\u001b[0m, in \u001b[0;36mtraining_loop\u001b[1;34m(dataloader, net, loss_fn, optimiser, verbosity, device)\u001b[0m\n\u001b[0;32m     23\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# perform a forward pass to compute the outputs of the net\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m pred \u001b[38;5;241m=\u001b[39m net(X)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# calculate the loss between the outputs of the net and the desired outputs\u001b[39;00m\n\u001b[0;32m     29\u001b[0m loss_val \u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\n",
            "File \u001b[1;32mc:\\Users\\maxlu\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\maxlu\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[1;32mIn[2], line 26\u001b[0m, in \u001b[0;36mDeepNeuralNet.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# loop through the layers to produce the output of the hidden network\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 26\u001b[0m   x \u001b[38;5;241m=\u001b[39m layer(x)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# TODO: produce the output of the network from the intermediate output of the last hidden layer\u001b[39;00m\n\u001b[0;32m     29\u001b[0m output_before_activation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(x)\n",
            "File \u001b[1;32mc:\\Users\\maxlu\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\maxlu\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\maxlu\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import optuna\n",
        "import numpy as np\n",
        "\n",
        "# Define the objective function for Optuna\n",
        "def objective(trial):\n",
        "    # Search space: number of neurons in each layer\n",
        "    num_layers = trial.suggest_int(\"num_layers\", 2, 4)  # Search between 2 and 4 layers\n",
        "    hidden_layer_profile = [\n",
        "        trial.suggest_int(f\"w_2_{i}\", 20, 100, step=10) for i in range(num_layers)\n",
        "    ]\n",
        "    \n",
        "    # Search space for learning rate\n",
        "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-3, 2e-2)\n",
        "\n",
        "    net = DeepNeuralNet(input_width=8, hidden_layer_profile=hidden_layer_profile, output_width=8, output_activation=nn.Sigmoid()).to(DEVICE)\n",
        "    \n",
        "    optimiser = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
        "    \n",
        "    # Train and get the loss\n",
        "    loss = train(training_dataloader, net, loss_fn, optimiser, epochs=400, verbosity=2)\n",
        "\n",
        "    w_sum = np.sum(num_layers) /100\n",
        "    \n",
        "    return loss + w_sum # Optuna will minimize this loss\n",
        "\n",
        "# Run Optuna study\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=50)  # Try 50 different configurations\n",
        "\n",
        "# Get the best trial\n",
        "best_trial = study.best_trial\n",
        "print(\"Best configuration:\")\n",
        "print(f\"Layers: {best_trial.params['num_layers']}\")\n",
        "print(f\"Neurons per layer: {[best_trial.params[f'w_2_{i}'] for i in range(best_trial.params['num_layers'])]}\")\n",
        "print(f\"Best learning rate: {best_trial.params['learning_rate']}\")\n",
        "print(f\"Best loss: {best_trial.value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtlSN3BgDObv"
      },
      "source": [
        "### Section Takeaway\n",
        "\n",
        "We have seen that neural networks can be constructed as directed graphs of more elementary building blocks (shallow and deep nets).\n",
        "\n",
        "We have also introduced the training loop for a neural network that uses much of PyTorch machinery to perform gradient descent.\n",
        "\n",
        "We have used the above to train networks that learn a Boolean function. We observed that not all networks can learn all functions, and that the number of neurons and their arrangement (or, more precisely, trainable weights and their role within the network) influence the ability of a network to learn to approximate a function. Experimenting, we got the intuitive feel of the notion of *representational power*.\n",
        "\n",
        "Using all that has been learned, we can now go and train networks that are perhaps more suitable for real-world applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgPbCBVh-X4A"
      },
      "source": [
        "## Image Classification with DNNs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-XRALwj3OQO"
      },
      "source": [
        "Most machine learning workflows involve working with data, creating models, optimizing model parameters, and saving the trained models. This section introduces you to a complete ML workflow implemented in PyTorch, with links to learn more about each of these concepts.\n",
        "\n",
        "We will use the FashionMNIST dataset to train a neural network that predicts if an input image belongs to one of the following classes: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, or Ankle boot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QE2rLR_3UYN"
      },
      "source": [
        "### Working with Data\n",
        "\n",
        "PyTorch has two primitives to work with data: `torch.utils.data.DataLoader` and `torch.utils.data.Dataset`. `Dataset` stores the samples and their corresponding labels, and `DataLoader` wraps an iterable around the `Dataset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "UrJOdfWg3fyO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import Compose, ToTensor, Lambda\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXhX0V6x3fOv"
      },
      "source": [
        "PyTorch offers domain-specific libraries such as TorchText, TorchVision, and TorchAudio, all of which include datasets. For this tutorial, we will be using a TorchVision dataset.\n",
        "\n",
        "The `torchvision.datasets` module contains `Dataset` objects for many real-world vision data like CIFAR, COCO. In this tutorial, we use the `FashionMNIST` dataset. Every TorchVision `Dataset` includes two arguments: `transform` and `target_transform`, to modify the samples and labels respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlvqVUin3sJK",
        "outputId": "fc681a65-2397-4740-b03c-b98eff06e2a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 13.2MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 208kB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.92MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 23.4MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Download training data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=Compose([\n",
        "      ToTensor(),\n",
        "      Lambda(lambda x: torch.flatten(x, start_dim=0))\n",
        "    ]),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=Compose([\n",
        "      ToTensor(),\n",
        "      Lambda(lambda x: torch.flatten(x, start_dim=0))\n",
        "    ]),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oPKr9xq30BL"
      },
      "source": [
        "We pass the `Dataset` as an argument to `DataLoader`. This wraps an iterable over our dataset, and supports automatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJWNfm3t4wDC",
        "outputId": "36a7de98-2098-41b7-90f7-37c939b8e6ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([64, 784])\n",
            "Shape of y: torch.Size([64]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size, pin_memory=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size, pin_memory=True)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcuLF_FcYtQS"
      },
      "source": [
        "We can also have a quick peek at the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "klZSCJp2YsdR",
        "outputId": "5d3e2ce1-b6b9-427a-edd8-32da6a181d17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of input tensor: [64, 784]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label:  9\n"
          ]
        }
      ],
      "source": [
        "images, labels = next(iter(train_dataloader))\n",
        "print('Shape of input tensor:', list(images.shape))\n",
        "ii = torch.reshape(images[0],(28,28))\n",
        "plt.imshow(ii, cmap='gray')\n",
        "plt.show()\n",
        "print('Label: ', int(labels[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1sUIuMb37Is"
      },
      "source": [
        "### Creating Models\n",
        "As we have in previous sections when learning Boolean functions, to define a neural network in PyTorch we create a class that inherits from `nn.Module`. We define the layers of the network in the `__init__` function (the constructor) and specify how data will pass through the network in the forward function. To accelerate operations in the neural network, we move it to the GPU if available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMpr9W3b34sp"
      },
      "outputs": [],
      "source": [
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "net = DeepNeuralNet(input_width=28 * 28, hidden_layer_profile=[512, 512], output_width=10).to(device)\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuUe4bYz4sKm"
      },
      "source": [
        "### Optimising Model Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5KVrwn14vwB"
      },
      "source": [
        "As illustrated before, to train a model, we need a loss function and an optimiser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDYCYpIM4t2n"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimiser = torch.optim.SGD(net.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ks-kMVbH45MH"
      },
      "source": [
        "The training loop from above will serve us well even in our current tasks. We also check the model’s performance against the test dataset to ensure it is learning -- in order to do so, we re-define the `testing_loop` function we used to learn Boolean functions in the new context of image classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_HRTMXS42fX"
      },
      "outputs": [],
      "source": [
        "def testing_loop(dataloader, net,):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = net(X)\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    return correct / size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riWKYtev5MGt"
      },
      "source": [
        "The training process is conducted over several iterations (epochs). During each epoch, the model learns parameters to make better predictions. We print the model's accuracy and loss at each epoch; we would like to see the accuracy increase and the loss decrease with every epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sdns1QKu5LK4"
      },
      "outputs": [],
      "source": [
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    training_loop(train_dataloader, net, loss_fn, optimiser)\n",
        "    validation_accuracy = testing_loop(train_dataloader, net)\n",
        "    print(f\"Validation Accuracy: {validation_accuracy:.2%}\\n\")\n",
        "print(\"Training Done!\")\n",
        "\n",
        "testing_accuracy = testing_loop(test_dataloader, net)\n",
        "print(f\"\\nTest Accuracy: {testing_accuracy:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Owb56aZqSk4x"
      },
      "source": [
        "We get an accuracy around 65 % which is much better than the 10 % we would get from random guessing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErIgSMFqLw_p"
      },
      "source": [
        "**Exercise.** Play around with different learning setups. Modifying just the learning rate and the number of epochs, how high can you take the validation accuracy? While doing so, do you also observe similar improvements in the test accuracy? (Remember that you can adjust the verbosity level not to have to read through long listings)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlxBrCs6L3tb"
      },
      "outputs": [],
      "source": [
        "net = DeepNeuralNet(input_width=28 * 28, hidden_layer_profile=[512, 512], output_width=10).to(device)\n",
        "optimiser = torch.optim.SGD(net.parameters(), lr=1e-3)\n",
        "train(train_dataloader, net, loss_fn, optimiser, epochs=5, epoch_frequency=1, verbosity=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hdz2qeSpQll8"
      },
      "source": [
        "**Exercise.** Play around with the architecture of the neural network that you train. Adding more layers and more neurons, can you take the test performance even higher than in the previous exercise?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZbTBJlKRJQu"
      },
      "outputs": [],
      "source": [
        "# define the architecture of your neural network\n",
        "best_net = DeepNeuralNet(input_width=28 * 28, hidden_layer_profile=[ None ], output_width=10).to(device)\n",
        "optimiser = torch.optim.SGD(best_net.parameters(), lr=1e-3)\n",
        "print(best_net)\n",
        "\n",
        "# train it\n",
        "train(train_dataloader, best_net, loss_fn, optimiser, epochs, verbosity=3)\n",
        "\n",
        "# test it\n",
        "testing_accuracy = testing_loop(test_dataloader, best_net)\n",
        "print(f\"\\nTest Accuracy: {testing_accuracy:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPpFsu8F59xI"
      },
      "source": [
        "### Saving and Loading Models\n",
        "Quite often you want to save your model, either to be later deployed in practice (on a website or in a mobile device, for example), or to be able to evaluate it later, in a different workflow. A common way to save a model is to serialise the internal state dictionary (containing the model parameters)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iH2lJVkk5ROQ"
      },
      "outputs": [],
      "source": [
        "torch.save(net.state_dict(), \"model.pth\")\n",
        "print(\"Saved PyTorch Model State to model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aei7Jih6FyU"
      },
      "source": [
        "The process for loading a model includes re-creating the model structure and loading the state dictionary into it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pa519J6y6JR8"
      },
      "outputs": [],
      "source": [
        "model = DeepNeuralNet(28 * 28, [512, 512], 10)\n",
        "model.load_state_dict(torch.load(\"model.pth\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMSKxvms6ROa"
      },
      "source": [
        "This model can now be used to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQFBDr7V6QOm"
      },
      "outputs": [],
      "source": [
        "classes = [\n",
        "    \"T-shirt/top\",\n",
        "    \"Trouser\",\n",
        "    \"Pullover\",\n",
        "    \"Dress\",\n",
        "    \"Coat\",\n",
        "    \"Sandal\",\n",
        "    \"Shirt\",\n",
        "    \"Sneaker\",\n",
        "    \"Bag\",\n",
        "    \"Ankle boot\",\n",
        "]\n",
        "\n",
        "model.eval()\n",
        "x, y = test_data[0][0], test_data[0][1]\n",
        "with torch.no_grad():\n",
        "    pred = model(x)\n",
        "    predicted, actual = classes[pred.argmax(0)], classes[y]\n",
        "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "DtlSN3BgDObv"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
